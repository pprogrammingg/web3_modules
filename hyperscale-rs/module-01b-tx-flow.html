<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transaction Flow: User to Finality</title>
    <link rel="stylesheet" href="../shared/styles.css">
</head>
<body>
    <nav class="site-home-bar">
        <a href="../index.html" class="site-home-link" aria-label="Home">üè† Home</a>
    </nav>
    <div class="course-content">
        <div class="course-header">
            <h1>Transaction Flow: User to Finality</h1>
            <div class="course-meta">
                <span>‚è±Ô∏è Duration: ~30 min</span>
                <span>üìä Difficulty: Basic</span>
                <span>üéØ Hyperscale-rs Specific</span>
            </div>
        </div>

        <div class="section">
            <h2>Why This Diagram?</h2>
            <p>Hyperscale-rs is a <strong>consensus layer</strong>, not a full blockchain stack. This diagram shows the end-to-end path of a transaction from the user to finality, and which parts Hyperscale touches versus which are outside (e.g. wallet, Radix Engine semantics, network client). Hover over each step to see which crates implement it.</p>
            <p><strong>Where BFT comes in:</strong> Steps 1‚Äì6 get the tx into the system (user, network, node, cross-shard determination, <span data-glossary="mempool">mempool</span>, and for cross-shard txs, travel to involved <span data-glossary="sharding">shards</span>). <strong>BFT consensus runs from step 7</strong> (proposer selection) through step 11 (block committed) <em>per <span data-glossary="sharding">shard</span></em>. Steps 12‚Äì14 are execution, then (for cross-shard) coordination &amp; composition, then finality.</p>
        </div>

        <div class="section">
            <h2>End-to-End Transaction Flow</h2>
            <div class="tx-flow">
                <div class="tx-flow-legend">
                    <span><span class="dot-hyperscale"></span> Hyperscale-rs (consensus layer)</span>
                    <span><span class="dot-outside"></span> Outside Hyperscale (wallet, engine, network)</span>
                </div>
                <div class="tx-flow-diagram">
                    <!-- 1. User -->
                    <div class="flow-step outside">
                        <div class="flow-step-lane outside"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">1. User signs transaction</div>
                            <div class="flow-step-desc">Wallet creates and signs a transaction (e.g. transfer, smart contract call).</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <p class="outside-note">Not in Hyperscale ‚Äî wallet / client application.</p>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 2. Submit to network -->
                    <div class="flow-step outside">
                        <div class="flow-step-lane outside"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">2. Transaction submitted to network</div>
                            <div class="flow-step-desc">RPC or gateway sends the signed tx to a node (or broadcast).</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <p class="outside-note">Network client / RPC ‚Äî outside. <code>production</code> receives on the node side.</p>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 3. Node receives -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">3. Node receives transaction</div>
                            <div class="flow-step-desc">Runner turns incoming bytes into <code>Event</code>s and feeds the node.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/production" target="_blank" rel="noopener">production</a> ‚Äî I/O, turns network into events</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî NodeStateMachine receives events</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 4. Cross-shard determination -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">4. Cross-shard determination</div>
                            <div class="flow-step-desc">The tx is analyzed for which <strong><span data-glossary="nodeid">NodeIDs</span></strong> (components, resources, packages, accounts) it reads or writes. If those NodeIDs belong to <strong>more than one <span data-glossary="sharding">shard</span></strong>, the tx is classified as a <strong><span data-glossary="cross-shard transaction">cross-shard transaction</span></strong>.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî routing, shard mapping</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/core" target="_blank" rel="noopener">core</a> ‚Äî Event/Action types</li>
                                </ul>
                                <p class="outside-note">Shard assignment of NodeIDs is defined by the protocol / Radix Engine; consensus uses it to decide single- vs cross-shard.</p>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 5. Mempool -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">5. <span data-glossary="mempool">Mempool</span> (per <span data-glossary="sharding">shard</span>)</div>
                            <div class="flow-step-desc"><strong>Single-<span data-glossary="sharding">shard</span>:</strong> tx goes into that shard's <span data-glossary="mempool">mempool</span>. <strong>Cross-shard:</strong> the receiving node enqueues the tx in <strong>its own shard‚Äôs mempool</strong> (if the tx touches that shard). For other involved shards, the tx is sent in step 6 and nodes there enqueue it in their mempools ‚Äî so the tx ends up in the mempool of every involved shard.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/mempool" target="_blank" rel="noopener">mempool</a> ‚Äî transaction pool</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî composes mempool</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/core" target="_blank" rel="noopener">core</a> ‚Äî Event/Action types</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 6. (Cross-shard) Tx travels to involved shards -->
                    <div id="step-6" class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">6. (Cross-shard) Tx travels to each involved shard</div>
                            <div class="flow-step-desc">For cross-shard txs, the receiving node (or the routing layer) <strong>sends</strong> the tx (or sub-transactions) to the nodes of every other involved <span data-glossary="sharding">shard</span>. Those nodes enqueue it in <strong>their</strong> shard‚Äôs mempool. A <strong>coordinator</strong> (one of these shards or a designated role) is chosen so that later (step 13) it can drive prepare/commit/abort in a fixed order. Each shard then runs steps 7‚Äì12 for its part.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/provisions" target="_blank" rel="noopener">provisions</a> ‚Äî centralized provision coordination; drives cross-shard flow</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/production" target="_blank" rel="noopener">production</a> ‚Äî cross-shard messaging (libp2p Gossipsub on shard-scoped topics)</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî composes provisions, coordination state</li>
                                </ul>
                                <p class="outside-note">In Hyperscale-rs, the node sends the tx to other shards via the production network layer: <strong>libp2p Gossipsub</strong> with shard-scoped topics (e.g. <code>hyperscale/{msg_type}/shard-{id}/1.0.0</code>). RPC is only for client‚Üínode submission.</p>
                                <p class="outside-note">livelock crate handles cross-shard deadlock detection and prevention.</p>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 7. Proposer selection -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">7. Proposer selection (per <span data-glossary="sharding">shard</span>, per round)</div>
                            <div class="flow-step-desc">BFT starts here. Each <span data-glossary="sharding">shard</span> runs its own BFT instance with its own <span data-glossary="view">view/round</span>. The proposer (block leader) is chosen deterministically from the shard's validator set (e.g. round-robin by <strong>validator identity</strong> or <span data-glossary="view">view number</span> modulo validator count).</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/bft" target="_blank" rel="noopener">bft</a> ‚Äî view, leader election</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/core" target="_blank" rel="noopener">core</a> ‚Äî traits, time</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/types" target="_blank" rel="noopener">types</a> ‚Äî block, validator set</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 8. Block proposal -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">8. Block proposal</div>
                            <div class="flow-step-desc">Proposer builds block (header + txs from <span data-glossary="mempool">mempool</span>), broadcasts header; validators receive it.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/bft" target="_blank" rel="noopener">bft</a> ‚Äî proposal logic</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/types" target="_blank" rel="noopener">types</a> ‚Äî Block, BlockHeader</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî routes ProposalTimer, etc.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 9. Validators vote -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">9. Validators vote</div>
                            <div class="flow-step-desc">Validators in this <span data-glossary="sharding">shard</span>'s BFT instance <strong>validate</strong> the block (e.g. check it extends from the parent QC, payload and hashes are valid, and it obeys consensus rules) and, if valid, sign a vote. 2f+1 votes are required for a quorum (BFT fault model: n = 3f+1 nodes per <span data-glossary="sharding">shard</span>).</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/bft" target="_blank" rel="noopener">bft</a> ‚Äî block validation logic and voting; collects votes</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/types" target="_blank" rel="noopener">types</a> ‚Äî Block, Vote, signatures</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 10. QC formed -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">10. Quorum certificate (QC) formed</div>
                            <div class="flow-step-desc">How a QC is formed: <strong>(i)</strong> Proposer for H built and broadcast the block header (step 8). <strong>(ii)</strong> Validators validated and voted; votes are broadcast to the shard (step 9). <strong>(iii)</strong> When any validator has 2f+1 valid votes, it requests QC build; that node‚Äôs <code>latest_qc</code> is set to the new QC. <strong>(iv)</strong> The QC is not sent as a separate message‚Äîthe next proposer has it by forming it from votes or from a received block header. <strong>(v)</strong> The next proposer builds block H+1 with <code>parent_qc</code> = QC(H) and broadcasts H+1; everyone then sees QC(H) in H+1‚Äôs header.</div>
                            <div class="flow-step-popup">
                                <h4>Crates &amp; code</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/bft" target="_blank" rel="noopener">bft</a> ‚Äî vote collection, QC build request, <code>latest_qc</code>, block build with <code>parent_qc</code></li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/types" target="_blank" rel="noopener">types</a> ‚Äî <code>BlockHeader.parent_qc</code> (block.rs:75), <code>QuorumCertificate</code></li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/core" target="_blank" rel="noopener">core</a> ‚Äî <code>Action::PersistAndBroadcastVote</code> (action.rs:444), <code>Action::VerifyAndBuildQuorumCertificate</code> (action.rs:113)</li>
                                </ul>
                                <p style="margin: 0.5rem 0 0 0; font-size: 0.85rem;"><strong>Locations:</strong> BFT state emits vote broadcast in <code>bft/src/state.rs</code> (‚âà2575), emits QC build in <code>bft/src/state.rs</code> (‚âà2765); <code>on_qc_formed</code> sets <code>latest_qc</code> (state.rs:3583); <code>latest_qc</code> from received header (state.rs:1597); block build uses <code>latest_qc</code> as <code>parent_qc</code> (state.rs:1013‚Äì1017, 1147, 1182‚Äì1208). <code>vote_set.build_qc</code> in <code>bft/src/vote_set.rs</code> (307).</p>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 11. Block committed -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">11. Block committed</div>
                            <div class="flow-step-desc"><strong>Who:</strong> Every validator <strong>node</strong> in the shard (not only the proposer). Once the commit rule is satisfied (e.g. two-chain: this block and the next have QCs), each node commits. <strong>Commit mechanics:</strong> (1) accept the block as final, (2) append it to the local chain, (3) trigger execution (run the transactions in the block), (4) trigger persistence (write to storage). All honest nodes do this in sync with the agreed order.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî CommitBlock action, composition</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/bft" target="_blank" rel="noopener">bft</a> ‚Äî commit rule</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/core" target="_blank" rel="noopener">core</a> ‚Äî Action::CommitBlock</li>
                                </ul>
                                <p class="outside-note">‚ÄúNode‚Äù = any validator running the NodeStateMachine in this shard; the proposer is one of them. All commit the same block in the same order.</p>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 12. Execution -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">12. Execution</div>
                            <div class="flow-step-desc">Transactions in the block are executed (per <span data-glossary="sharding">shard</span>). Hyperscale runs the execution state machine; semantics (e.g. Radix Engine) may be external.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/execution" target="_blank" rel="noopener">execution</a> ‚Äî execution state machine; cross-shard 2PC coordination</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/engine" target="_blank" rel="noopener">engine</a> ‚Äî Radix Engine integration for smart contract execution</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî composes execution</li>
                                </ul>
                                <p class="outside-note">Execution semantics (Radix Engine) are in engine crate / vendor; BFT only orders and coordinates.</p>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 13. (Cross-shard) Coordination & composition -->
                    <div id="step-13" class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">13. (Cross-shard) Coordination &amp; composition</div>
                            <div class="flow-step-desc">For <strong>cross-shard</strong> txs, <strong>atomic composability</strong> works like this. <strong>Communication &amp; order:</strong> Shards talk in a fixed protocol order (e.g. one <strong>coordinator <span data-glossary="sharding">shard</span></strong> drives <span data-glossary="two-phase commit">2PC</span>). It sends <strong>prepare</strong> to each involved shard; each shard runs its part (<span data-glossary="prepare phase">lock/reserve</span>, no visible state change yet) and replies yes/no. If <strong>all</strong> vote yes, coordinator sends <strong>commit</strong>; otherwise <strong>abort</strong>. <strong>State updates:</strong> Only after the decision, each shard applies updates in an <strong>agreed order</strong> (e.g. by shard ID or coordinator order) ‚Äî commit = apply state; abort = release locks. That way every node sees the same composed outcome and no shard commits while another aborts. Single-shard txs skip this.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/provisions" target="_blank" rel="noopener">provisions</a> ‚Äî centralized provision coordination for cross-shard txs</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/execution" target="_blank" rel="noopener">execution</a> ‚Äî transaction execution with cross-shard <span data-glossary="two-phase commit">2PC</span> coordination</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî composes provisions, execution, core</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/core" target="_blank" rel="noopener">core</a> ‚Äî Event/Action for coordination</li>
                                </ul>
                                <p class="outside-note">See module ‚ÄúCross-Shard Transactions‚Äù for 2PC and provision coordination details.</p>
                            </div>
                        </div>
                    </div>
                    <div class="flow-step-arrow">‚Üì</div>

                    <!-- 14. Finality -->
                    <div class="flow-step hyperscale">
                        <div class="flow-step-lane hyperscale"></div>
                        <div class="flow-step-body">
                            <div class="flow-step-title">14. Finality &amp; persistence</div>
                            <div class="flow-step-desc">BFT gives one-round finality (no reorg after QC). State/storage may be in-node or external.</div>
                            <div class="flow-step-popup">
                                <h4>Crates</h4>
                                <ul>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/bft" target="_blank" rel="noopener">bft</a> ‚Äî finality rule</li>
                                    <li><a href="https://github.com/flightofthefox/hyperscale-rs/tree/main/crates/node" target="_blank" rel="noopener">node</a> ‚Äî state, persistence</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <p style="margin-top: 1rem; font-size: 0.9rem; color: var(--text2);"><strong>Tip:</strong> Hover over a step to see which crates implement it. Popup closes when you move the cursor away.</p>

            <div class="section section-tight">
                <h3>Clarifying the flow (multi-shard and proposer)</h3>
                <p>A few common points to keep straight:</p>
                <ul>
                    <li><strong>Proposer is per shard, per round.</strong> The node that receives the tx (e.g. via RPC) belongs to some shard, but that does <em>not</em> make it the proposer. Proposer is chosen <strong>deterministically per shard</strong> (e.g. round-robin by view). So for shard A, ‚Äúthe proposer for this round‚Äù is one validator in A; for shard B, it‚Äôs one validator in B. They are different nodes. The receiving node just ingests the tx (steps 3‚Äì6); the tx lands in <strong>mempool(s)</strong> of every involved shard. The proposer <strong>pulls</strong> from its shard‚Äôs mempool when its <strong>ProposalTimer</strong> fires and builds a block ‚Äî it doesn‚Äôt ‚Äúreceive the tx‚Äù specially.</li>
                    <li><strong>Who enqueues in which mempool?</strong> The node that receives the tx (e.g. RPC node) belongs to one shard. It can enqueue the tx in <strong>its own shard‚Äôs mempool</strong> (if the tx touches that shard). It cannot directly enqueue into another shard‚Äôs mempool ‚Äî that shard‚Äôs mempool lives on that shard‚Äôs nodes. So for cross-shard: the receiving node enqueues in its shard‚Äôs mempool (step 5) and <strong>sends</strong> the tx (or sub-tx) to the other involved shards (step 6). Nodes in those shards receive it and enqueue it in <strong>their</strong> mempools. So the tx ends up in the mempools of every involved shard, but ‚Äúenqueue in mempools of every involved shard‚Äù is done by the receiving node for its shard and by the other shards‚Äô nodes when they receive the tx.</li>
                    <li><strong>Tx does not ‚Äúbroadcast until it lands on the proposer.‚Äù</strong> Once the tx is in the mempools (as above), each shard‚Äôs proposer pulls from <em>that shard‚Äôs</em> mempool when its ProposalTimer fires and builds a block. The proposer broadcasts the <strong>block (header)</strong> to <strong>validators in that shard only</strong>, not to other shards.</li>
                    <li><strong>Proposer sends a block to its shard, not ‚Äúthe block‚Äù to all shards.</strong> The proposer for <strong>shard S</strong> builds <strong>one block</strong> (header with <code>parent_qc</code> = QC for previous block in S‚Äôs chain, plus txs from S‚Äôs mempool). It broadcasts that block (or header; full block is assembled via gossip) to <strong>validators in shard S only</strong>. So: one block per shard, broadcast inside that shard. Other shards have their own proposers and their own blocks at the same ‚Äúlogical‚Äù time.</li>
                    <li><strong>Voting and execution order.</strong> Validators in <strong>that shard</strong> vote on the <strong>block</strong> (BFT: do we agree on this block?). That yields a QC for that shard‚Äôs block. Then the block is committed (step 11). <strong>After</strong> commit, execution runs (step 12): the transactions in the block are run (e.g. via Radix Engine). So: BFT agrees on block ‚Üí commit ‚Üí then execute. There is no ‚Äúeach shard votes on validity and reports to the proposer‚Äù in the sense of one global proposer; each shard votes on its own block and gets its own QC.</li>
                    <li><strong>Cross-shard: no single ‚Äúproposer finalizes with all shards‚Äô QCs.‚Äù</strong> Each shard has its <strong>own</strong> chain and its <strong>own</strong> QC. For a cross-shard tx, the tx (or sub-tx) is in each involved shard‚Äôs mempool; each shard may include it in its block; each shard runs BFT (propose ‚Üí vote ‚Üí QC ‚Üí commit) and then execution. <strong>Atomicity across shards</strong> is achieved by <strong>2PC</strong> (step 13): a coordinator drives prepare ‚Üí all shards prepare (lock/reserve) and reply yes/no ‚Üí commit or abort ‚Üí each shard applies state in an agreed order. So ‚Äúall report success‚Äù is the 2PC prepare phase; then commit/abort and apply. There is no single block that ‚Äúincludes the votes (QC) of each shard‚Äù ‚Äî each shard‚Äôs block has its own QC in the <em>next</em> block‚Äôs header (the normal two-chain rule per shard).</li>
                    <li><strong>What is the coordinator?</strong> The <strong>2PC coordinator</strong> (step 13) is <strong>one of the involved shards</strong> (or a designated role), not a separate server. Validators in that shard run the logic that sends prepare to each involved shard, collects yes/no, then sends commit or abort. So it's a shard (and its validators) with a coordination role for that tx. Separately, the <strong>ProvisionCoordinator</strong> (provisions crate) is a sub-state machine that runs on <em>every</em> node: it tracks provisions from other shards until quorum is reached ("all shards done" in the sense of having enough provisions), then emits events so execution can proceed ‚Äî it is not elected and is not a single designated node.</li>
                </ul>
            </div>

            <div class="info section-tight">
                <h3>How atomic composability works (cross-shard)</h3>
                <p>The diagram emphasizes that cross-shard atomicity depends on <strong>correct order</strong> of communication and <strong>when</strong> state updates are applied:</p>
                <ol class="list-nested">
                    <li><strong>Coordinator</strong> (one shard or a designated role) sends <strong>prepare</strong> to every involved shard in a defined order.</li>
                    <li>Each shard <strong>prepares</strong>: runs its part of the tx (e.g. <span data-glossary="prepare phase">reserve/lock</span>), but does <em>not</em> make the state update visible yet. It replies yes or no.</li>
                    <li>When <strong>all</strong> have voted yes, coordinator sends <strong>commit</strong>; otherwise it sends <strong>abort</strong> to all.</li>
                    <li><strong>State updates</strong>: Each shard then applies updates in an <strong>agreed order</strong> (e.g. by shard ID, or the order the coordinator specifies). On commit, each applies its state change; on abort, each releases locks. No shard commits while another aborts ‚Äî the protocol order ensures a single composed outcome.</li>
                </ol>
                <p style="margin-bottom: 0;">So shards communicate in this <strong>fixed protocol order</strong> (prepare phase ‚Üí decision ‚Üí apply phase), and state updates happen only after the decision, in the same order everywhere. That is what makes the cross-shard tx <strong>atomic</strong> and <strong>composable</strong>.</p>
            </div>

            <div class="info section-tight">
                <h3>Two coordinators: 2PC coordinator and ProvisionCoordinator</h3>
                <p>Cross-shard flow uses two different ‚Äúcoordinator‚Äù ideas; they do different jobs.</p>
                <ul class="list-nested">
                    <li><strong>2PC coordinator (step 13).</strong> This is <strong>one of the involved shards</strong> (or a designated role), not a separate server. Validators in that shard run the logic that: sends <strong>prepare</strong> to each involved shard (in a fixed order), collects yes/no, then sends <strong>commit</strong> (if all yes) or <strong>abort</strong>. So it‚Äôs the shard that ‚Äúasks whether everyone is ready‚Äù and then decides go or cancel. Implementation: <code>execution</code> crate (cross-shard 2PC coordination).</li>
                    <li><strong>ProvisionCoordinator (provisions crate).</strong> This is a <strong>sub-state machine on every node</strong>, not a single elected node. It tracks <strong>provisions</strong>: after a shard commits its part of a cross-shard tx, it sends a signed proof (a ‚Äúprovision‚Äù) to the other shards. ProvisionCoordinator on each node keeps a checklist: ‚ÄúFor this tx, do we have quorum of provisions from shard 1? From shard 2? ‚Ä¶‚Äù When it has enough provisions from every required (earlier) shard, it emits events so execution can proceed. So it‚Äôs the ‚Äúchecklist‚Äù that checks ‚Äúdid we get enough proofs from the prerequisite shards?‚Äù ‚Äî not the prepare yes/no votes (that‚Äôs 2PC). Implementation: <code>provisions</code> crate.</li>
                </ul>
                <p style="margin-top: 0.75rem;"><strong>How is the order fixed?</strong> ‚ÄúThe protocol fixes an order‚Äù means a <strong>deterministic rule</strong> that all nodes follow (e.g. by shard ID, or by the order of NodeIDs in the tx), so everyone agrees on which shard is first, second, third. The <strong>2PC coordinator</strong> does not choose that order; it <strong>drives</strong> the protocol in that pre-agreed order (sends prepare in that order, then commit/abort, then state updates in that order). So: the protocol defines the order; the coordinator executes the protocol in that order.</p>
                <p style="margin-top: 0.75rem; margin-bottom: 0;"><strong>How is order determined for composite (cross-shard) transactions?</strong> In the <strong>Radix manifest</strong>, instructions are in a fixed order (e.g. withdraw from A ‚Üí split ‚Üí put in staking vault ‚Üí put in LP ‚Üí return IOUs). The <strong>Radix Engine</strong> runs those instructions sequentially when executing; data dependencies are implicit (instruction 2 sees the result of instruction 1). For <strong>Hyperscale (consensus)</strong>, the transaction is turned into a <strong>RoutableTransaction</strong> by <strong>instruction analysis</strong> (<code>crates/types/src/transaction.rs</code>): the manifest is walked and every NodeId read or written is collected into <code>declared_reads</code> and <code>declared_writes</code>. Those are <strong>sets</strong> (deduplicated); <strong>instruction order is not preserved</strong> at the consensus layer. Shard sets are then derived: <strong>consensus_shards</strong> = unique shards of <code>declared_writes</code>; <strong>provisioning_shards</strong> = shards of <code>declared_reads</code> that are not write shards. Both are stored in <strong>BTreeSet</strong>s, so the <strong>protocol order is by ShardGroupId</strong> (numeric). So: we do <em>not</em> derive "Account_A shard first, then Staking_VAULT shard, then LP shard" from the manifest order‚Äîwe get a deterministic order by <strong>shard ID</strong>. Prerequisites are enforced by <strong>provisions</strong>: a shard that needs remote state (reads from another shard) must receive quorum of provisions from that shard before it can complete; which shards are "required" is the set of all other participating shards. Parallelism: each shard runs BFT and execution independently; cross-shard atomicity is then 2PC (prepare all in shard-ID order, then commit/abort) and provisioning (everyone waits for provisions from every other participating shard). Refs: <code>crates/types/src/topology.rs</code> (<code>consensus_shards</code>, <code>provisioning_shards</code>, <code>all_shards_for_transaction</code>), <code>crates/types/src/transaction.rs</code> (<code>analyze_instructions_v1</code> / <code>analyze_instructions_v2</code>).</p>
            </div>

            <div class="section section-tight">
                <h2>Complex manifest example (intermediate)</h2>
                <p>For a <strong>complex Radix manifest example</strong> and how provisioning and coordination work for it, see the intermediate module <a href="module-05-cross-shard.html">Cross-Shard Transactions in Hyperscale-rs</a>. That module includes a full example (withdraw ‚Üí split ‚Üí stake ‚Üí LP ‚Üí deposit), shard mapping, ProvisionCoordinator checklist, and a harder quiz.</p>
                <ul style="margin: 0.5rem 0 0 1.25rem; display: none;">
                    <li><code>declared_writes</code>: Account_A, Staking_VAULT, LP_COMPONENT (and possibly resource/vault NodeIds)</li>
                    <li><code>declared_reads</code>: any read-only touched nodes (e.g. package, resource type)</li>
                </ul>
                <p><strong>Shard mapping (topology):</strong> Each NodeId maps to a shard via <code>shard_for_node(node_id, num_shards)</code> (e.g. hash-based). Suppose Account_A ‚Üí shard 1, Staking_VAULT ‚Üí shard 2, LP_COMPONENT ‚Üí shard 3. Then:</p>
                <ul style="margin: 0.5rem 0 0 1.25rem;">
                    <li><strong>consensus_shards</strong> = {1, 2, 3} (unique shards of declared_writes), ordered as <strong>1, 2, 3</strong> (BTreeSet by ShardGroupId).</li>
                    <li><strong>provisioning_shards</strong> = read-only shards (if any) not in that set.</li>
                    <li><strong>all_shards_for_transaction</strong> = consensus ‚à™ provisioning, again in shard-ID order.</li>
                </ul>
                <p><strong>Provisioning and coordination:</strong></p>
                <ul style="margin: 0.5rem 0 0 1.25rem;">
                    <li>The tx is sent to shards 1, 2, 3 (step 6). Each shard runs BFT and execution for its part. When a shard commits its block containing this tx, it produces a <strong>StateProvision</strong> (signed proof of the state it wrote) and sends it to the other shards.</li>
                    <li><strong>ProvisionCoordinator</strong> on each node (e.g. on shard 3) keeps a checklist: ‚ÄúDo we have quorum of provisions from shard 1? From shard 2?‚Äù When it has provisions from <strong>every</strong> other participating shard (required_shards), it emits <strong>ProvisioningComplete</strong> so execution can proceed (or finalize). So shard 3 cannot ‚Äúfinish‚Äù its view of the cross-shard tx until it has proofs from shards 1 and 2‚Äîthat‚Äôs the prerequisite.</li>
                    <li><strong>2PC coordinator</strong> (one of shards 1, 2, or 3) drives prepare ‚Üí collect yes/no ‚Üí commit or abort in <strong>shard-ID order</strong> (1, 2, 3), not in manifest order. State updates are applied in that same order after the decision.</li>
                </ul>
                <p style="margin-bottom: 0;">So: the <strong>manifest</strong> defines the logical flow (withdraw ‚Üí split ‚Üí stake ‚Üí LP ‚Üí deposit); the <strong>engine</strong> runs it sequentially; Hyperscale uses <strong>shard ID order</strong> for coordination and <strong>provisions</strong> to enforce ‚Äúeveryone has proof from everyone else‚Äù before completion.</p>
            </div>

        <div class="section">
            <h2>Concepts in the Flow</h2>
            <ul>
                <li><strong><span data-glossary="sharding">Shards</span></strong> ‚Äî The network is split into shards; each shard has its own validators and ordering. Your tx is assigned to a shard (e.g. by account or payload). <strong>Each shard has its own BFT consensus state</strong>: its own <span data-glossary="view">view number</span>, its own block chain, and its own proposer per <span data-glossary="view">round</span>. So shard A and shard B run independent BFT instances; they don‚Äôt share a single global ‚Äúview.‚Äù</li>
                <li><strong>Proposer (block leader)</strong> ‚Äî For each <span data-glossary="view">round (view)</span>, one validator in that shard is the proposer. It builds the block and broadcasts it; the other validators in the shard vote. <strong>Proposer selection</strong> is deterministic: typically a function of the current <span data-glossary="view">view number</span> and the shard's validator set, e.g. <code>proposer = validators[view % validators.len()]</code> (round-robin by index) or by validator identity. That way everyone agrees on who the leader is without extra messages.</li>
                <li><strong>Validator identity (consensus)</strong> ‚Äî In the consensus layer, ‚Äúwhich validator‚Äù is identified by a validator ID (from the node‚Äôs public key or protocol assignment). That identity is used for the validator set, proposer selection (e.g. round-robin), and routing. In Hyperscale-rs, types that refer to ‚Äúwhich node‚Äù in the sense of ‚Äúwhich validator‚Äù use this.</li>
                <li><strong><span data-glossary="nodeid">NodeID</span> in Radix (on-chain)</strong> ‚Äî In Radix, <strong><span data-glossary="nodeid">NodeID</span></strong> often means something different: it refers to <strong>addresses of on-chain entities</strong> ‚Äî components, resources, packages, accounts. Those are ‚Äúnodes‚Äù in the ledger‚Äôs state graph (e.g. a component instance, a resource type). So: validator identity = which machine runs consensus; NodeID/address in Radix = which component, resource, or package you‚Äôre talking about on the ledger. Don‚Äôt confuse the two when reading Radix docs.</li>
                <li><strong><span data-glossary="cross-shard transaction">Cross-shard</span> transactions &amp; atomic composability</strong> ‚Äî If a tx reads or writes <span data-glossary="nodeid">NodeIDs</span> on <strong>different <span data-glossary="sharding">shards</span></strong>, it is cross-shard. It is sent to each involved shard (step 6); each shard runs BFT and execution for its part (steps 7‚Äì12). <strong>How shards communicate:</strong> In a defined <strong>order</strong> ‚Äî typically a <strong>coordinator <span data-glossary="sharding">shard</span></strong> (or role) sends <strong>prepare</strong> to all; each shard prepares (<span data-glossary="prepare phase">lock/reserve</span>, no visible state yet) and replies yes/no. If all vote yes, coordinator sends <strong>commit</strong>; else <strong>abort</strong> (see <span data-glossary="two-phase commit">2PC</span>). <strong>State updates:</strong> Each shard applies updates only <em>after</em> the decision, in an <strong>agreed order</strong> (e.g. by shard ID or coordinator order) ‚Äî commit = apply state, abort = release. That way every node sees the same composed outcome; no shard commits while another aborts. Atomic composability = one logical result across shards.</li>
                <li><strong>2PC coordinator vs ProvisionCoordinator</strong> ‚Äî See the ‚ÄúTwo coordinators‚Äù box above. In short: the <strong>2PC coordinator</strong> is one involved shard that asks ‚Äúeveryone ready?‚Äù (prepare), then sends commit or abort. The <strong>ProvisionCoordinator</strong> is a checklist on every node: ‚ÄúDo we have quorum of provisions (proofs) from each prerequisite shard?‚Äù so execution can proceed. The protocol fixes the order (e.g. by shard ID); the 2PC coordinator drives the protocol in that order.</li>
                <li><strong>Finality</strong> ‚Äî Once a block has a quorum certificate (2f+1 votes from that shard‚Äôs validators), it is final. No reorg under normal BFT assumptions.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Quiz: Transaction Flow</h2>
            <p>Answer based on the diagram and concepts above. Pass threshold: 70%.</p>
            <div id="quiz-tx-flow" class="quiz-container"></div>
        </div>

        <div class="navigation">
            <a href="module-01-overview.html" class="btn btn-secondary">‚Üê Previous Module</a>
            <button id="complete-module-btn" class="btn btn-primary">Mark as Complete</button>
            <a href="module-02-codebase-exploration.html" class="btn btn-primary">Next Module ‚Üí</a>
        </div>
    </div>

    <script src="../shared/course-data.js"></script>
    <script src="../shared/navigation.js"></script>
    <script src="../shared/glossary.js"></script>
    <script>
        initializeModulePage('basic-05b');
        const quizQuestions = [
            {
                question: "At which step does BFT consensus first participate in the transaction flow?",
                options: [
                    "Step 7 (Proposer selection)",
                    "Step 4 (Cross-shard determination)",
                    "Step 5 (Mempool)",
                    "Step 3 (Node receives)"
                ],
                correct: 0,
                explanation: "BFT runs from step 7 (proposer selection) through step 11 (block committed) per shard. Steps 1‚Äì6 are about getting the tx into the system (user, network, node, cross-shard determination, mempool, travel to shards)."
            },
            {
                question: "How is a transaction classified as cross-shard?",
                options: [
                    "If it is submitted from a wallet that has accounts on multiple shards",
                    "If the NodeIDs (components, resources, packages, accounts) it reads or writes belong to more than one shard",
                    "If it is larger than a size threshold defined by the coordinator",
                    "If it is the first transaction in a new block"
                ],
                correct: 1,
                explanation: "Cross-shard determination (step 4) analyzes which NodeIDs the tx reads or writes. If those NodeIDs belong to more than one shard, the tx is cross-shard."
            },
            {
                question: "Who defines which shard a NodeID (e.g. a component address) belongs to?",
                options: [
                    "Each validator decides locally to balance load",
                    "The protocol / Radix Engine; consensus uses that mapping to decide single- vs cross-shard",
                    "The proposer of the current block",
                    "The mempool crate in Hyperscale-rs"
                ],
                correct: 1,
                explanation: "Shard assignment of NodeIDs is defined by the protocol or Radix Engine; the consensus layer uses it to decide single- vs cross-shard, but does not define the mapping itself."
            },
            {
                question: "For a cross-shard transaction, where does the tx (or sub-transactions) go after cross-shard determination?",
                options: [
                    "Only to the shard that owns the first NodeID in the tx",
                    "To the mempools of every shard that owns a touched NodeID",
                    "To a single global mempool that then forwards to shards at commit time",
                    "Only to the coordinator shard until step 13"
                ],
                correct: 1,
                explanation: "Single-shard: one mempool. Cross-shard: the tx or sub-transactions are enqueued in the mempools of every involved shard (step 5), and in step 6 they travel to each involved shard so each can order and process its part."
            },
            {
                question: "How is the proposer (block leader) chosen for a given round in a shard?",
                options: [
                    "Validators vote in a separate election phase before each block",
                    "Deterministically from the current view number and the shard's validator set (e.g. round-robin by index or validator identity)",
                    "The validator that received the most transactions in the previous round",
                    "Random selection weighted by stake"
                ],
                correct: 1,
                explanation: "Proposer selection is deterministic: e.g. proposer = validators[view % validators.len()] or by validator identity. Everyone agrees on the leader without extra messages."
            },
            {
                question: "In the BFT fault model used per shard, how many nodes are required for a quorum and what is the total node count?",
                options: [
                    "Quorum = f+1, total n = 2f+1 (CFT style)",
                    "Quorum = 2f+1, total n = 3f+1 (BFT: up to f Byzantine)",
                    "Quorum = 3f+1, total n = 4f+1",
                    "Quorum = majority, total n = any"
                ],
                correct: 1,
                explanation: "BFT uses n = 3f+1 nodes per shard; up to f can be Byzantine. A quorum is 2f+1 votes; any two quorums overlap in at least one honest node."
            },
            {
                question: "When does a block become final in this flow?",
                options: [
                    "After it is proposed (step 8)",
                    "After 2f+1 validators have voted and a QC is formed (step 10); BFT gives one-round finality, no reorg after QC",
                    "Only after step 13 (coordination) for cross-shard txs",
                    "After execution (step 12) when state is persisted"
                ],
                correct: 1,
                explanation: "Once a block has a quorum certificate (2f+1 votes from that shard's validators), it is final. BFT gives one-round finality; no reorg under normal BFT assumptions. Execution and persistence follow."
            },
            {
                question: "In step 13 (cross-shard coordination), when do shards make their state updates visible?",
                options: [
                    "During the prepare phase (lock/reserve and apply in the same round)",
                    "Only after the coordinator's decision (commit or abort), in an agreed order; prepare phase does not make state visible",
                    "Each shard applies as soon as it has voted yes, before the coordinator decides",
                    "Only the coordinator shard applies; others send results to it"
                ],
                correct: 1,
                explanation: "Prepare = lock/reserve, no visible state change yet. State updates happen only after the decision (commit or abort), in an agreed order (e.g. by shard ID or coordinator order), so every node sees the same composed outcome."
            },
            {
                question: "What happens in the prepare phase of cross-shard 2PC?",
                options: [
                    "Each shard commits its state change and then notifies the coordinator",
                    "Each shard runs its part (e.g. reserve/lock), does not make the state update visible yet, and replies yes or no to the coordinator",
                    "The coordinator locks all shards' state and then executes the tx itself",
                    "Validators on each shard vote on the block again"
                ],
                correct: 1,
                explanation: "Coordinator sends prepare to each involved shard. Each shard prepares (reserve/lock), does not make state visible, and replies yes or no. Only after all vote yes does the coordinator send commit; otherwise abort."
            },
            {
                question: "Why must state updates (commit or release) be applied in an 'agreed order' across shards in step 13?",
                options: [
                    "To minimize network messages",
                    "So every node sees the same composed outcome and no shard commits while another aborts; the protocol order ensures a single logical result",
                    "Because the coordinator can only send one message at a time",
                    "To allow rollback if a validator crashes"
                ],
                correct: 1,
                explanation: "Applying in an agreed order (e.g. by shard ID or coordinator order) ensures a single composed outcome everywhere and prevents one shard committing while another aborts ‚Äî that is what makes the cross-shard tx atomic and composable."
            },
            {
                question: "Which of these is Hyperscale-rs responsible for, and which is outside?",
                options: [
                    "Hyperscale: consensus (order, BFT, cross-shard coordination). Outside: wallet, RPC client, Radix Engine execution semantics",
                    "Hyperscale: wallet and signing. Outside: consensus and execution",
                    "Hyperscale: Radix Engine semantics. Outside: BFT and mempool",
                    "Hyperscale: all steps 1‚Äì14. Outside: nothing"
                ],
                correct: 0,
                explanation: "Hyperscale-rs is a consensus layer: it handles ordering, BFT per shard, mempool, execution state machine wiring, and cross-shard coordination. Wallet, RPC/gateway, and execution semantics (e.g. Radix Engine) are outside."
            },
            {
                question: "In Radix terminology, what does 'NodeID' usually refer to in documentation?",
                options: [
                    "The validator node's identity used for proposer selection and routing in consensus",
                    "Addresses of on-chain entities (components, resources, packages, accounts) ‚Äî nodes in the ledger state graph",
                    "The hash of the current block",
                    "The shard identifier"
                ],
                correct: 1,
                explanation: "In Radix, NodeID often means on-chain entity addresses (components, resources, packages, accounts). Validator identity (which machine runs consensus) is a different concept ‚Äî don't confuse the two."
            },
            {
                question: "Do shard A and shard B share a single BFT view number and block chain?",
                options: [
                    "Yes; there is one global view and one chain across all shards",
                    "No; each shard has its own BFT consensus state (own view number, own block chain, own proposer per round); they run independent BFT instances",
                    "Only the coordinator shard has a view; others follow it",
                    "They share a view but have separate chains"
                ],
                correct: 1,
                explanation: "Each shard has its own BFT consensus state: its own view number, its own block chain, and its own proposer per round. Shard A and shard B run independent BFT instances; they don't share a single global view."
            },
            {
                question: "Which crate is primarily responsible for turning incoming network bytes into Events and feeding the node?",
                options: [
                    "bft",
                    "execution",
                    "production (I/O, turns network into events); node (NodeStateMachine) receives them",
                    "mempool"
                ],
                correct: 2,
                explanation: "Step 3: the runner (production) turns incoming bytes into Events and feeds the node; the NodeStateMachine in the node crate receives events."
            },
            {
                question: "For a single-shard transaction, which step is skipped that cross-shard transactions use?",
                options: [
                    "Step 4 (Cross-shard determination)",
                    "Steps 6 (Tx travels to involved shards) and 13 (Coordination & composition)",
                    "Step 7 (Proposer selection)",
                    "Step 10 (QC formed)"
                ],
                correct: 1,
                explanation: "Single-shard txs skip step 6 (tx travels to each involved shard) and step 13 (coordination & composition). Step 4 still runs (to determine it is single-shard); BFT steps 7‚Äì11 run once on that shard."
            },
            {
                question: "Where does execution semantics (e.g. what a Radix Engine instruction does) live relative to Hyperscale-rs?",
                options: [
                    "In the execution crate; Hyperscale-rs defines all semantics",
                    "In the bft crate as part of block validation",
                    "Hyperscale runs the execution state machine (execution crate), but semantics (e.g. Radix Engine) are outside this repo",
                    "In the mempool crate"
                ],
                correct: 2,
                explanation: "Hyperscale runs the execution state machine (execution crate); execution semantics ‚Äî what instructions and components do ‚Äî are defined outside (e.g. Radix Engine)."
            },
            {
                question: "What is the role of the coordinator in cross-shard 2PC?",
                options: [
                    "It executes the entire transaction and broadcasts the result to other shards",
                    "It sends prepare to each involved shard, collects yes/no votes, then sends commit if all yes or abort otherwise; it drives the fixed protocol order",
                    "It aggregates QCs from each shard into one global QC",
                    "It is the proposer for every involved shard"
                ],
                correct: 1,
                explanation: "The coordinator (one shard or role) sends prepare to all, collects yes/no, then sends commit or abort. It drives the fixed protocol order so state updates happen only after the decision in an agreed order."
            },
            {
                question: "After the coordinator sends 'abort', what do involved shards do?",
                options: [
                    "Apply their state changes anyway to maintain availability",
                    "Release locks / reserve; no state change is applied; the cross-shard tx has no visible effect",
                    "Vote again in a new round",
                    "Send their prepared state to the coordinator for rollback"
                ],
                correct: 1,
                explanation: "On abort, each shard releases locks; no state update is applied. The cross-shard tx has no visible effect ‚Äî atomic all-or-nothing."
            },
            {
                question: "Which step ensures that a cross-shard transaction is atomic (all shards commit or all abort)?",
                options: [
                    "Step 5 (Mempool) by ordering the tx the same way on every shard",
                    "Step 10 (QC formed) because the QC binds all shards",
                    "Step 13 (Coordination & composition): 2PC prepare ‚Üí commit or abort ‚Üí state updates in agreed order",
                    "Step 14 (Finality) when state is persisted"
                ],
                correct: 2,
                explanation: "Step 13 (coordination & composition) ensures atomicity: prepare on all, then commit (all apply) or abort (all release); state updates in agreed order so no shard commits while another aborts."
            },
            {
                question: "Why is the QC for a block included in the *next* block's header rather than broadcast separately?",
                options: [
                    "To reduce message size",
                    "The next block's proposer includes the previous block's QC in its header; the QC proves that block was agreed by 2f+1 validators, and all can verify it when they receive the next block",
                    "Because the BFT crate does not support separate QC messages",
                    "To allow the proposer to change the QC before inclusion"
                ],
                correct: 1,
                explanation: "Votes are broadcast; when a validator has 2f+1 votes it requests QC build; the QC is stored as latest_qc and included as parent_qc in the next block‚Äôs header (types/block.rs BlockHeader.parent_qc; bft/state.rs for building block with latest_qc). No separate QC message‚Äîeveryone sees QC(H) when they receive block H+1."
            },
            {
                question: "Validator identity (consensus layer) is used for which of the following?",
                options: [
                    "Identifying on-chain components and resources in the ledger state",
                    "Validator set membership, proposer selection (e.g. round-robin), and routing messages to the right validator node",
                    "Signing transactions on behalf of users",
                    "Deriving the shard ID for a given NodeID"
                ],
                correct: 1,
                explanation: "Validator identity identifies 'which validator' in the consensus layer: validator set, proposer selection (e.g. round-robin), and routing. It is not NodeID (on-chain addresses) or user signing."
            }
        ];
        initializeQuiz('quiz-tx-flow', quizQuestions, 70);
    </script>
</body>
</html>
