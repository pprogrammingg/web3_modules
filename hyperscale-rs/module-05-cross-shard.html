<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cross-Shard Transactions in Hyperscale-rs</title>
    <link rel="stylesheet" href="../shared/styles.css">
</head>
<body>
    <nav class="site-home-bar">
        <a href="../index.html" class="site-home-link" aria-label="Home">üè† Home</a>
    </nav>
    <div class="course-content">
        <div class="course-header">
            <h1>Cross-Shard Transactions in Hyperscale-rs</h1>
            <div class="course-meta">
                <span>‚è±Ô∏è Duration: 1.5‚Äì2 hours</span>
                <span>üìä Difficulty: Intermediate</span>
                <span>üéØ Hyperscale-rs Specific</span>
            </div>
        </div>

        <div class="section">
            <h2>Learning Objectives</h2>
            <ul>
                <li>Understand 2PC and provision coordination in hyperscale-rs</li>
                <li>Map a complex Radix manifest to shards, provisioning, and coordination</li>
                <li>See how protocol order (ShardGroupId) differs from manifest instruction order</li>
                <li>Understand livelock prevention in cross-shard flow</li>
                <li>Trace cross-shard flow in the codebase (types, topology, execution, provisions)</li>
            </ul>
        </div>

        <div class="info section-tight">
            <h2>How atomic composability works (cross-shard)</h2>
            <p>The diagram in <a href="module-01b-tx-flow.html" target="_blank" rel="noopener noreferrer">Transaction Flow</a> emphasizes that cross-shard atomicity depends on <strong>correct order</strong> of communication and <strong>when</strong> state updates are applied:</p>
            <ol class="list-nested">
                <li><strong>Coordinator</strong> (one shard or a designated role) sends <strong>prepare</strong> to every involved shard in a defined order.</li>
                <li>Each shard <strong>prepares</strong>: runs its part of the tx (e.g. <span data-glossary="prepare phase">reserve/lock</span>), but does <em>not</em> make the state update visible yet. It replies yes or no.</li>
                <li>When <strong>all</strong> have voted yes, coordinator sends <strong>commit</strong>; otherwise it sends <strong>abort</strong> to all.</li>
                <li><strong>State updates</strong>: Each shard then applies updates in an <strong>agreed order</strong> (e.g. by shard ID, or the order the coordinator specifies). On commit, each applies its state change; on abort, each releases locks. No shard commits while another aborts ‚Äî the protocol order ensures a single composed outcome.</li>
            </ol>
            <p style="margin-bottom: 0;">So shards communicate in this <strong>fixed protocol order</strong> (prepare phase ‚Üí decision ‚Üí apply phase), and state updates happen only after the decision, in the same order everywhere. That is what makes the cross-shard tx <strong>atomic</strong> and <strong>composable</strong>.</p>
        </div>

        <div class="section">
            <h2>Two coordinators: 2PC coordinator and ProvisionCoordinator</h2>
            <p>Cross-shard flow uses two different ‚Äúcoordinator‚Äù ideas:</p>
            <ul>
                <li><strong>2PC coordinator (<a href="module-01b-tx-flow.html#step-13" target="_blank" rel="noopener noreferrer">step 13 in the tx flow</a>).</strong> This is <strong>one of the involved shards</strong> (or a designated role), not a separate server. Validators in that shard run the logic that: sends <strong>prepare</strong> to each involved shard (in a fixed order), collects yes/no, then sends <strong>commit</strong> (if all yes) or <strong>abort</strong>. So it‚Äôs the shard that ‚Äúasks whether everyone is ready‚Äù and then decides go or cancel. Implementation: <code>execution</code> crate (cross-shard 2PC coordination).</li>
                <li><strong>ProvisionCoordinator (provisions crate).</strong> This is a <strong>sub-state machine on every node</strong>, not a single elected node. It tracks <strong>provisions</strong>: after a shard commits its part of a cross-shard tx, it sends a signed proof (a "provision") to the other shards. ProvisionCoordinator on each node keeps a checklist: ‚ÄúFor this tx, do we have quorum of provisions from shard 1? From shard 2? ‚Ä¶‚Äù When it has enough from every required shard, it emits <strong>ProvisioningComplete</strong> so execution can proceed. So it‚Äôs the ‚Äúchecklist‚Äù for ‚Äúdid we get enough proofs from the prerequisite shards?‚Äù ‚Äî not the prepare yes/no (that‚Äôs 2PC).</li>
            </ul>
            <p><strong>How is the order fixed?</strong> "The protocol fixes an order" means a <strong>deterministic rule</strong> that all nodes follow (e.g. by shard ID, or by the order of NodeIDs in the tx), so everyone agrees on which shard is first, second, third. The <strong>2PC coordinator</strong> does not choose that order; it <strong>drives</strong> the protocol in that pre-agreed order (sends prepare in that order, then commit/abort, then state updates in that order). So: the protocol defines the order; the coordinator executes the protocol in that order.</p>
            <p><strong>How is order determined for composite (cross-shard) transactions?</strong> In the <strong>Radix manifest</strong>, instructions are in a fixed order (e.g. withdraw from A ‚Üí split ‚Üí put in staking vault ‚Üí put in LP ‚Üí return IOUs). The <strong>Radix Engine</strong> runs those instructions sequentially when executing; data dependencies are implicit (instruction 2 sees the result of instruction 1). For <strong>Hyperscale (consensus)</strong>, the transaction is turned into a <strong>RoutableTransaction</strong> by <strong>instruction analysis</strong> (<code>crates/types/src/transaction.rs</code>): the manifest is walked and every NodeId read or written is collected into <code>declared_reads</code> and <code>declared_writes</code>. Those are <strong>sets</strong> (deduplicated); <strong>instruction order is not preserved</strong> at the consensus layer. Shard sets are then derived: <strong>consensus_shards</strong> = unique shards of <code>declared_writes</code>; <strong>provisioning_shards</strong> = shards of <code>declared_reads</code> that are not write shards. Both are stored in <strong>BTreeSet</strong>s, so the <strong>protocol order is by ShardGroupId</strong> (numeric). So: we do <em>not</em> derive "Account_A shard first, then Staking_VAULT shard, then LP shard" from the manifest order‚Äîwe get a deterministic order by <strong>shard ID</strong>. Prerequisites are enforced by <strong>provisions</strong>: a shard that needs remote state (reads from another shard) must receive quorum of provisions from that shard before it can complete; which shards are "required" is the set of all other participating shards. Parallelism: each shard runs BFT and execution independently; cross-shard atomicity is then 2PC (prepare all in shard-ID order, then commit/abort) and provisioning (everyone waits for provisions from every other participating shard). Refs: <code>crates/types/src/topology.rs</code> (<code>consensus_shards</code>, <code>provisioning_shards</code>, <code>all_shards_for_transaction</code>), <code>crates/types/src/transaction.rs</code> (<code>analyze_instructions_v1</code> / <code>analyze_instructions_v2</code>).</p>
            <p>See the <a href="module-01b-tx-flow.html" target="_blank" rel="noopener noreferrer">Transaction Flow</a> diagram for the full step-by-step from user to finality.</p>
        </div>

        <div class="section">
            <h2>Example: complex Radix manifest and Hyperscale provisioning</h2>
            <p>Consider a composite transaction that splits funds from a user account into staking and liquidity:</p>
            <pre class="code-block-light"><code># Simplified Radix-style manifest (conceptual)
1. CallMethod(Account_A, "withdraw", XRD, amount)     # ‚Üí NodeID_Account_A, vault
2. TakeFromWorktop(XRD, amount)
3. SplitBucket(amount1, amount2)                        # worktop
4. CallMethod(StakingVault, "stake", bucket1)          # ‚Üí NodeID_Staking_VAULT
5. CallMethod(LiquidityPool, "contribute", bucket2)    # ‚Üí NodeID_LP_COMPONENT
6. CallMethod(Account_A, "deposit", staking_IOU)        # ‚Üí NodeID_Account_A
7. CallMethod(Account_A, "deposit", lp_IOU)             # ‚Üí NodeID_Account_A</code></pre>
            <p><strong>Radix Engine:</strong> Runs these instructions <strong>in order</strong>. Data flow is implicit (e.g. step 4 uses the bucket from step 3; step 6‚Äì7 deposit what step 4‚Äì5 returned).</p>
            <p><strong>Instruction analysis (Hyperscale):</strong> The manifest is walked and every NodeId read or written is collected into <code>declared_reads</code> and <code>declared_writes</code> (<code>crates/types/src/transaction.rs</code>: <code>analyze_instructions_v1</code> / <code>analyze_instructions_v2</code>). The result is <strong>sets</strong> (deduplicated); <strong>instruction order is not preserved</strong> at the consensus layer. Assume:</p>
            <ul>
                <li><code>declared_writes</code>: Account_A, Staking_VAULT, LP_COMPONENT (and possibly resource/vault NodeIds)</li>
                <li><code>declared_reads</code>: any read-only touched nodes (e.g. package, resource type)</li>
            </ul>
            <p><strong>Shard mapping (topology):</strong> Each NodeId maps to a shard via <code>shard_for_node(node_id, num_shards)</code> (<code>crates/types/src/topology.rs</code>). Suppose Account_A ‚Üí shard 1, Staking_VAULT ‚Üí shard 2, LP_COMPONENT ‚Üí shard 3. Then:</p>
            <ul>
                <li><strong>consensus_shards</strong> = {1, 2, 3} (unique shards of declared_writes), ordered as <strong>1, 2, 3</strong> (BTreeSet by ShardGroupId).</li>
                <li><strong>provisioning_shards</strong> = read-only shards (if any) not in that set.</li>
                <li><strong>all_shards_for_transaction</strong> = consensus ‚à™ provisioning, again in shard-ID order.</li>
            </ul>
            <p><strong>Provisioning and coordination:</strong></p>
            <ul>
                <li>The tx is sent to shards 1, 2, 3 (<a href="module-01b-tx-flow.html#step-6" target="_blank" rel="noopener noreferrer">step 6 in the tx flow</a>). Each shard runs BFT and execution for its part. When a shard commits its block containing this tx, it produces a <strong>StateProvision</strong> (signed proof of the state it wrote) and sends it to the other shards.</li>
                <li><strong>ProvisionCoordinator</strong> on each node (e.g. on shard 3) keeps a checklist: ‚ÄúDo we have quorum of provisions from shard 1? From shard 2?‚Äù When it has provisions from <strong>every</strong> other participating shard (<code>required_shards</code> in <code>TxRegistration</code>), it emits <strong>ProvisioningComplete</strong> so execution can proceed. So shard 3 cannot ‚Äúfinish‚Äù its view of the cross-shard tx until it has proofs from shards 1 and 2‚Äîthat‚Äôs the prerequisite.</li>
                <li><strong>2PC coordinator</strong> (one of shards 1, 2, or 3) drives prepare ‚Üí collect yes/no ‚Üí commit or abort in <strong>shard-ID order</strong> (1, 2, 3), not in manifest order. State updates are applied in that same order after the decision.</li>
            </ul>
            <p>So: the <strong>manifest</strong> defines the logical flow (withdraw ‚Üí split ‚Üí stake ‚Üí LP ‚Üí deposit); the <strong>engine</strong> runs it sequentially; Hyperscale uses <strong>shard ID order</strong> for coordination and <strong>provisions</strong> to enforce ‚Äúeveryone has proof from everyone else‚Äù before completion.</p>
        </div>

        <div class="section">
            <h2>Order: manifest vs protocol</h2>
            <p>As above: instruction order is not preserved at consensus; protocol order is by <strong>ShardGroupId</strong>. Prerequisites are enforced by provisions; <code>required_shards</code> is the set of all other participating shards (<code>start_cross_shard_execution</code> in <code>crates/execution/src/state.rs</code>).</p>
        </div>

        <div class="section">
            <h2>Concepts in the Flow</h2>
            <ul>
                <li><strong><span data-glossary="sharding">Shards</span></strong> ‚Äî The network is split into shards; each shard has its own validators and ordering. Your tx is assigned to a shard (e.g. by account or payload). <strong>Each shard has its own BFT consensus state</strong>: its own <span data-glossary="view">view number</span>, its own block chain, and its own proposer per <span data-glossary="view">round</span>. So shard A and shard B run independent BFT instances; they don't share a single global "view."</li>
                <li><strong>Proposer (block leader)</strong> ‚Äî For each <span data-glossary="view">round (view)</span>, one validator in that shard is the proposer. It builds the block and broadcasts it; the other validators in the shard vote. <strong>Proposer selection</strong> is deterministic: typically a function of the current <span data-glossary="view">view number</span> and the shard's validator set, e.g. <code>proposer = validators[view % validators.len()]</code> (round-robin by index) or by validator identity. That way everyone agrees on who the leader is without extra messages.</li>
                <li><strong>Validator identity (consensus)</strong> ‚Äî In the consensus layer, "which validator" is identified by a validator ID (from the node's public key or protocol assignment). That identity is used for the validator set, proposer selection (e.g. round-robin), and routing. In Hyperscale-rs, types that refer to "which node" in the sense of "which validator" use this.</li>
                <li><strong><span data-glossary="nodeid">NodeID</span> in Radix (on-chain)</strong> ‚Äî In Radix, <strong><span data-glossary="nodeid">NodeID</span></strong> often means something different: it refers to <strong>addresses of on-chain entities</strong> ‚Äî components, resources, packages, accounts. Those are "nodes" in the ledger's state graph (e.g. a component instance, a resource type). So: validator identity = which machine runs consensus; NodeID/address in Radix = which component, resource, or package you're talking about on the ledger. Don't confuse the two when reading Radix docs.</li>
                <li><strong><span data-glossary="cross-shard transaction">Cross-shard</span> transactions &amp; atomic composability</strong> ‚Äî If a tx touches <span data-glossary="nodeid">NodeIDs</span> on <strong>different <span data-glossary="sharding">shards</span></strong>, it is cross-shard: sent to each involved shard (step 6); each runs BFT and execution for its part (steps 7‚Äì12). Atomicity across shards: <span data-glossary="two-phase commit">2PC</span> (prepare ‚Üí commit/abort) and provisions (see "How atomic composability works" and "Two coordinators" above).</li>
                <li><strong>2PC coordinator vs ProvisionCoordinator</strong> ‚Äî See the "Two coordinators" section above. In short: the <strong>2PC coordinator</strong> is one involved shard that asks "everyone ready?" (prepare), then sends commit or abort. The <strong>ProvisionCoordinator</strong> is a checklist on every node: "Do we have quorum of provisions (proofs) from each prerequisite shard?" so execution can proceed. The protocol fixes the order (e.g. by shard ID); the 2PC coordinator drives the protocol in that order.</li>
                <li><strong>Finality</strong> ‚Äî Once a block has a quorum certificate (2f+1 votes from that shard's validators), it is final. No reorg under normal BFT assumptions.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Quiz: Cross-shard and provisioning</h2>
            <p>Answer based on the content above. Pass threshold: 70%.</p>
            <div id="quiz-cross-shard" class="quiz-container"></div>
        </div>

        <div class="navigation">
            <a href="../intermediate/module-01-sharding.html" class="btn btn-secondary">‚Üê Previous Module</a>
            <button id="complete-module-btn" class="btn btn-primary">Mark as Complete</button>
            <a href="module-06-execution.html" class="btn btn-primary">Next Module ‚Üí</a>
        </div>
    </div>

    <script src="../shared/course-data.js"></script>
    <script src="../shared/navigation.js"></script>
    <script src="../shared/glossary.js"></script>
    <script>
        initializeModulePage('intermediate-03');
        const quizQuestions = [
            {
                question: "In the complex manifest example (Account_A ‚Üí shard 1, Staking_VAULT ‚Üí shard 2, LP ‚Üí shard 3), in what order does the 2PC coordinator drive prepare and state updates?",
                options: [
                    "In manifest order: Account_A first, then Staking_VAULT, then LP",
                    "In ShardGroupId (numeric) order: 1, then 2, then 3",
                    "In the order the provisions arrive at each node",
                    "The coordinator chooses the order dynamically per transaction"
                ],
                correct: 1,
                explanation: "Protocol order is fixed by BTreeSet iteration over ShardGroupIds‚Äîi.e. numeric shard ID order (1, 2, 3). Manifest instruction order is not preserved at the consensus layer."
            },
            {
                question: "When building a RoutableTransaction from a Radix UserTransaction, what happens to the order of instructions in the manifest?",
                options: [
                    "Instruction order is preserved in declared_reads and declared_writes so shards execute in manifest order",
                    "Instruction order is lost; only sets of NodeIds (declared_reads, declared_writes) are extracted, and shard order is by ShardGroupId",
                    "Only the first and last instructions determine shard order",
                    "The Radix Engine sends the manifest order to Hyperscale in a separate field"
                ],
                correct: 1,
                explanation: "Instruction analysis (analyze_instructions_v1/v2) collects NodeIds into HashSets; result is deduplicated sets. Consensus uses BTreeSet of shards (by ShardGroupId), so instruction order is not preserved."
            },
            {
                question: "For a cross-shard tx, required_shards in TxRegistration (ProvisionCoordinator) is set to what?",
                options: [
                    "Only the shards that appear earlier than the local shard in the manifest",
                    "Only the write shards (consensus_shards)",
                    "All other participating shards (all_shards_for_transaction minus local shard)",
                    "Only the 2PC coordinator shard"
                ],
                correct: 2,
                explanation: "In start_cross_shard_execution, remote_shards = participating_shards minus local_shard; TxRegistration.required_shards is that set. So we need provisions from every other participating shard before we can complete."
            },
            {
                question: "In the complex manifest example, before shard 3 (LP) can complete its part of the cross-shard tx, what must ProvisionCoordinator on shard 3's nodes have?",
                options: [
                    "Only a commit message from the 2PC coordinator",
                    "Quorum of StateProvision proofs from shard 1 and quorum of StateProvision proofs from shard 2",
                    "The full block contents from shards 1 and 2",
                    "A single aggregated signature from the coordinator shard"
                ],
                correct: 1,
                explanation: "ProvisionCoordinator tracks provisions (StateProvision) from each required shard. When it has quorum of provisions from every required shard (here 1 and 2), it emits ProvisioningComplete so execution can proceed."
            },
            {
                question: "Why does Hyperscale use ShardGroupId order (e.g. BTreeSet) for cross-shard coordination instead of manifest instruction order?",
                options: [
                    "Manifest order is not available when the transaction is validated",
                    "So that all nodes deterministically agree on the same order without exchanging extra messages; instruction order is not exposed to consensus",
                    "Because the Radix Engine does not define instruction order",
                    "To minimize the number of provisions sent"
                ],
                correct: 1,
                explanation: "Consensus needs a deterministic order everyone can compute locally. Instruction analysis produces sets of NodeIds, not an ordered list; topology derives shard sets in BTreeSet order (ShardGroupId). So protocol order is by shard ID."
            },
            {
                question: "A transaction has declared_writes on shards 2 and 5, and declared_reads (only) on shards 2 and 7. Which shards are consensus_shards and which are provisioning_shards?",
                options: [
                    "Consensus: {2, 5, 7}; provisioning: {}",
                    "Consensus: {2, 5}; provisioning: {7}",
                    "Consensus: {2}; provisioning: {5, 7}",
                    "Consensus: {2, 7}; provisioning: {5}"
                ],
                correct: 1,
                explanation: "consensus_shards = unique shards of declared_writes ‚Üí {2, 5}. provisioning_shards = shards of declared_reads that are not in write shards ‚Üí shard 7 (read-only), not 2 (2 is already a write shard)."
            }
        ];
        initializeQuiz('quiz-cross-shard', quizQuestions, 70);
    </script>
</body>
</html>
